\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{fancyvrb}

\mode<beamer>{%
\setbeamertemplate{footline}[text line]{%
  \parbox{\linewidth}{\vspace*{-64pt}https://cobordism.com}}}
\mode<beamer>{%
\setbeamertemplate{navigation symbols}{}}

\title{Who's Afraid of the Big Bad ceval Loop?}
\subtitle{Me}
\author{Moshe Zadka -- https://cobordism.com}
\date{2020}

\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\frame{\titlepage}

\begin{frame}
\frametitle{What is not the eval loop?}
\end{frame}

The CPython interpreter,
as its name implies,
is writen in C.
It contains several parts,
which we will not delve into today.
It has the parser and byte compiler,
which produce Python byte code.
It also has the implementation of
native objects,
such as integers as strings,
as C
"extension classes".

The eval loop does not do any parsing or byte-code compilation.
Those steps must have been done before:
usually,
Python will do them for us
(although we can force it to execute on a sequence
of byte codes).
It also does not have any of Python's
*object*
semantics:
it does not know that one integer divided by another must sometimes
result in a floating point number,
or that strings are immutable.

The semantics of Python's
*objects*
are part of the class definitions:
this is where integer division
or string modification
(resulting in an error)
are implemented.

\begin{frame}
\frametitle{What is the eval loop?}

\begin{lstlisting}
for (;;) {
    /* ... */
    /* This is a lie */
    opcode = (*next_instr) & 0xff;
    oparg = (*next_instr) >> 8;
    next_instr++;
    /* ... */
    switch (opcode) {
        /* ... */
    }
}
\end{lstlisting}

\end{frame}

In the middle between those parts,
something must
*actually execute*
the byte-code.
When we have a sequence of byte codes,
something must inspect the code,
decide which operation to do,
and do it.

As this description might imply,
this is one of the most performance critical parts
in the CPython interpreter.
The eval loop has seen many generations of iterations,
and has evolved together with CPU architecture.
This is the reason for the white lie above:
the actual code is a few layers of macros,
including an ifdef based on the machine architecture.


\begin{frame}[fragile]
\frametitle{Code objects}

\begin{lstlisting}
>>> (lambda x:None).__code__
<code object <lambda> at 0x7fb7b67725d0, file "<stdin>", line 1>
\end{lstlisting}

\end{frame}

Code objects are usually found in functions,
as the \verb|__code__| attribute.

\begin{frame}
\frametitle{Python bytecode}

\begin{lstlisting}
>>> (lambda a, b: a + b
... ).__code__.co_code
b'|\x00|\x01\x17\x00S\x00'
>>> dis.dis(_)
          0 LOAD_FAST                0 (0)
          2 LOAD_FAST                1 (1)
          4 BINARY_ADD
          6 RETURN_VALUE
\end{lstlisting}

\end{frame}

Python bytecode are...
bytes.
They come in sequence of two:
a code,
and then an argument.
For codes that do not need an argument,
the argument will be NIL,
the 0 byte.

\begin{frame}
\frametitle{Frame}

Where the eval happens
\end{frame}

Python bytecode always happens in the context of a
{\em frame}.
A frame gives context to code.
Calling a function will
*create*
a frame.
Naturally,
distinct calls to the function
will generate distinct frames.
This is how recursion can work.

Frames are explicit,
though not documented,
in Python.
The current frame can always be retrieved by
\verb|sys._getframe()|.
While it should never be done in
"real"
code,
it is a great exercise to play with it.
It also serves as a useful thing in debuggers
and other coding tools.

\begin{frame}
\frametitle{Frame contents}

Code and context
\end{frame}

Since code evaluation always happen in the context of a
*frame*,
the frame holds the code.
It also holds several links to namespaces:
the locals, the globals, and the builtins.
Finally,
frames are
*nested*:
a frame will have a link to the "outer",
or "back",
frame.



\begin{frame}
\frametitle{Frame inspection}

\begin{lstlisting}
>>> def foo(): bar()
... 
>>> def bar():
...     global frm;frm = sys._getframe()
... 
>>> frm.f_code.co_name
'bar'
>>> frm.f_back.f_code.co_name
'foo'
>>> frm.f_back.f_back.f_code.co_name
'<module>'
\end{lstlisting}
\end{frame}

The frames have a link to the "back frame",
where a "return" statement or a "yield" statement would go.
This explicit structure of frames is an essential part of allowing
generators and other more complicated control structures.
They also have a link to the code object.
Frames also have an \verb|f_lasti| property,
which contains an index to the last instruction.
This is the variable that the eval loop increments.

\begin{frame}
\frametitle{Stack machine}

\begin{lstlisting}
PyObject **stack_pointer;
/* ... */
#define BASIC_PUSH(v)     (*stack_pointer++ = (v))
#define BASIC_POP()       (*--stack_pointer)
\end{lstlisting}
\end{frame}

The basic semantics of Python's byte code are those of a stack machine.
The operations all operate on a stack:
it can be pushed or popped,
and operations happen on the stack.
The stack space is part of the frame.
This means that a frame suspended,
for example when yielding,
still has the stack.

\begin{frame}
\frametitle{Stack operations}

\begin{lstlisting}
#define TOP()             (stack_pointer[-1])
#define PEEK(n)           (stack_pointer[-(n)])
#define SET_TOP(v)        (stack_pointer[-1] = (v))
\end{lstlisting}
\end{frame}

The Python implementation defines some macros for highly
efficient stack operations. This is important,
since much of what Python does involves manipulatig the
stack.

\begin{frame}[fragile]
\frametitle{The Loop}

\begin{lstlisting}
PyObject *_PyEval_EvalFrameDefault(PyFrameObject *f, int throwflag)
{
    /* ... */
    /* Small lie */
    next_instr = f->f_code->co_code + f->f_lasti;
    for (;;) {
        /* ... */
        /* Small lie */
    fast_next_opcode:
        opcode = (*next_instr) & 0xff;
        oparg = (*next_instr) >> 8;
        next_instr++;
        switch (opcode) {
            /* ... */
        }
    }
    error:
        /* ... */
}
\end{lstlisting}
\end{frame}

This is the Python eval loop.
We initialize the insturction pointer to the current place based
on the frame
(in case it needs to be resumed).
The lies are because I simplified the code, and made it less efficient,
but equivalent. Python is highly optimized, but that can obscure
the semantics.


\begin{frame}
\frametitle{The Switch}
\begin{lstlisting}
 switch (opcode) {
        /* ... */
        case TARGET(LOAD_FAST): {
            PyObject *value = GETLOCAL(oparg);
            if (value == NULL) {
                /* ... */
                goto error;
            }
            /* ... */
            goto fast_next_opcode;
        }
        /* ... */
        case TARGET(UNARY_NEGATIVE): {
            /* ... */
            continue;
        }
        /* ... */
}
\end{lstlisting}
\end{frame}

Every instruction handle finishes with one of three:
either it goes to an error, or it does a
"fast"
or
"slow"
dispatch.
"Fast" dispatches are those that skip various checks:
for example,
thread switches cannot happen between a "fast" dispatch
and the one that follows.

\begin{frame}
\frametitle{Eval Breaker}
\begin{lstlisting}
ceval->eval_breaker = 
   ceval->gil_drop_request |
   ceval->signals_pending | 
   ceval->pending.calls_to_do) | 
   ceval->pending.async_exc;
\end{lstlisting}
\end{frame}

These are lies!
The actual code is slightly more complicated since it has to
do this computation while carefully considering signals and threads,
so it uses all kinds of thread-safe primitives.
However, the actual computation is the same.

\frametitle{Eval Breaker}
\begin{lstlisting}
if (eval_breaker) {
    if (ceval->signals_pending) {
        handle_signals(runtime);
    }
    if (ceval->pending.calls_to_do) {
        make_pending_calls(runtime);
    }
    if (ceval->gil_drop_request) {
        drop_gil(ceval, tstate);
        /* Other threads may run now */
        take_gil(ceval, tstate);
    }
    if (tstate->async_exc != NULL) {
        _PyErr_SetNone(tstate, tstate->async_exc);
        goto error;
    }
}
\end{lstlisting}
\end{frame}

As always,
these are a bunch of dirty lies!
The actual code has a bunch of stuff for thread-safety,
and error handling.
It also skips eval-breakage in the presence of some opcodes
that would make handling signals cause issues:
we delay until we get to a nice,
regular opcode.

\begin{frame}
\frametitle{Signal handling}
\begin{lstlisting}
void trip_signal(int sig_num) {
     /* ... */
    PyRuntimeState *runtime = &_PyRuntime;
    PyThreadState *tstate = _PyRuntimeState_GetThreadState(runtime);
    runtime->ceval->signals_pending = 1
    runtime->ceval->eval_breaker = 1
    /* ... */
}
\end{lstlisting}
\end{frame}

This is a simplified version of Python's signal handler.
It sets the signals pending and eval breaker flags,
so that eventually,
the loop will handle the signals.

Signals are called in a
"safe"
place Python-wise,
but still somewhat arbitrary:
in theory,
we could be inside the evaluation of Python code inside
an \verb|__del__| that was called because a list index was set.


\begin{frame}
\frametitle{Global Interpreter Lock}

\begin{itemize}
\item Python bytecode evaluation
\end{itemize}
\end{frame}

The global interpreter lock,
on which many reams of ink have been spilled,
makes sure that Python can only evaluate Python bytecode
on only one thread.
This means that the beginning of the ceval loop,
somewhere at the top,
makes sure we have the lock.

However,
as we saw,
every few calls,
we do "eval breaker"
and do a drop/take of the GIL.
This means that if another thread is waiting in its
"take"
GIL then the drop will give it a chance to capture it.

\begin{frame}
\frametitle{GIL: Acquisition}
\begin{lstlisting}
MUTEX_LOCK(gil->mutex);
while (gil.locked)
    COND_TIMED_WAIT(gil->cond, gil->mutex, interval, timed_out);
    ceval.gil_drop_request = 1;
    ceval.eval_breaker = 1;
}
MUTEX_UNLOCK(gil->mutex);
MUTEX_LOCK(gil->switch_mutex);
gil.locked = 1
MUTEX_UNLOCK(gil->switch_mutex);
\end{lstlisting}
\end{frame}

This loop is the logic for acquiring the GIL.
We lock the mutex,
and then loop waiting for the condition variable to fire,
and the GIL to become unlocked.
Once it is unlocked,
we lock the GIL ourselves,
and we are done.


\begin{frame}
\frametitle{GIL: Release}
\begin{lstlisting}
MUTEX_LOCK(gil->mutex);
gil.locked = 0;
COND_SIGNAL(gil->cond);
MUTEX_UNLOCK(gil->mutex);
\end{lstlisting}
\end{frame}

Releasing the GIL is much more straightforward:
we do not have to wait,
we just set it to unlocked,
and fire the condition variable.

\begin{frame}
\frametitle{Takeaways}

\begin{itemize}
\item Python is not magic\pause
\item ...although it is definitely sufficiently advanced.
\end{itemize}
\end{frame}

CPython,
at the end of the day,
is just a program written in C.
It is possible to figure what is going on there.
That said,
remember that almost all code examples shown here were
bold-faced lies,
and the actual code pulls a lot of tricks to eke out performance.

\end{document}
